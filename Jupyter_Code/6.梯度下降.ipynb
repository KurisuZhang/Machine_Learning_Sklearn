{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 梯度下降法(Gradient-Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43gatLD5rzk_"
   },
   "source": [
    "## 原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bMEl9elFrzlD"
   },
   "source": [
    "### 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ryH6oWI1rzlF"
   },
   "source": [
    "   - 不是一个机器学习的算法\n",
    "   - 是一种基于搜索的最优化方法\n",
    "   - 作用: 最小化一个损失函数\n",
    "   - 梯度上升法: 最大化一个效用函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [](![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020/20200422204644.png)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "boxHRFs_rzlH"
   },
   "source": [
    "### 步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4WqUS3ForzlI"
   },
   "source": [
    "- 先找到目标函数,比如\n",
    "$$ y=x+b $$\n",
    "- 求解损失函数$J({\\theta})$, 比如用MSE,也叫标准差\n",
    "$$J=\\sum_{i=1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}$$ \n",
    "- 求损失函数的导数, 求解最小值或最大值\n",
    "$$J(\\theta)=\\frac{1}{2 m} \\sum_{i=1}^{m}\\left(y^{(i)}-\\hat{y}^{(i)}\\right)^{2}$$\n",
    "- 利用梯度公式,实现惩罚机制,更新$\\theta$\n",
    "$$\\theta = \\theta - \\alpha {f_{loss}}'(\\theta)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gzhHO2MOrzlU"
   },
   "source": [
    "### 注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85i-S2qQrzlW"
   },
   "source": [
    "* 学习率的问题\n",
    "* 要数据归一化\n",
    "* $X_0$ 添加一列为1\n",
    "* 某一函数有多个极值点: 解决方法-多次运行-随机化初始点\n",
    "* 将两次theta至变化小于1e-8时，可以认为到达了极值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOvvTTykrzlK"
   },
   "source": [
    "## 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量梯度下降法(BGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 批量梯度下降法是最原始的形式\n",
    "- 表达式递推到具有m个训练样本\n",
    "- 然后对损失函数求导\n",
    "$$\\frac{\\partial J\\left(\\Theta_{0}, \\Theta_{1}\\right)}{\\partial \\Theta_{j}}=\\frac{1}{m} \\sum_{i=1}^{m}\\left(H_{\\Theta}\\left(X^{(i)}\\right)-Y^{(i)}\\right) X_{j}^{(i)}$$  \n",
    "$$\\Theta_{j}=\\Theta_{i}-\\alpha \\frac{1}{m} \\sum_{i=1}^{m}\\left(H_{\\Theta}\\left(X^{(i)}\\right)-Y^{(i)}\\right) X_{j}^{(i)}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020/20200422205146.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优点：\n",
    "    1. 一次迭代是对所有样本进行计算，此时利用矩阵进行运算，实现了并行。\n",
    "    2. 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。\n",
    "    3. 当目标函数为凸函数时，批量梯度下降一定能够得到全局最优解\n",
    "- 缺点\n",
    "    1. 有时我们会遇到样本数目 m 很大的训练集合，如果有几十上百万，甚至上亿的训练样本。\n",
    "    2. 这意味着我们每执行一次批梯度下降算法，都要对m个样本进行求和。\n",
    "    3. 这样会导致，训练过程很慢，花费很长的时间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机梯度下降法(SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 随机梯度下降是每次迭代使用一个样本来对参数进行更新\n",
    "- 使得训练速度加快。\n",
    "$$\\frac{\\Delta J^{(i)}\\left(\\theta_{0}, \\theta_{1}\\right)}{\\theta_{j}}=\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}$$  \n",
    "$$\\theta_{j}:=\\theta_{j}-\\alpha\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) x_{j}^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/LinCheungS/PicGo_Image_Storage/master/2020/20200422205626.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\eta$ 称为学习率(learning rate)  \n",
    "- $\\eta$ 的取值影响获得最优解的速度  \n",
    "- $\\eta$ 取值不合适，甚至得不到最优解  \n",
    "- $\\eta$ 是梯度下降法的一个超参数  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优点：\n",
    "    1. 由于不是在全部训练数据上的损失函数，而是在每轮迭代中，随机优化某一条训练数据上的损失函数，这样每一轮参数的更新速度大大加快。\n",
    "- 缺点\n",
    "    1. 准确度下降。由于即使在目标函数为强凸函数的情况下，SGD仍旧无法做到线性收敛。\n",
    "    2. 可能会收敛到局部最优，由于单个样本并不能代表全体样本的趋势。\n",
    "    3. 不易于并行实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 小批量梯度下降法(MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法\n",
    "- batchsize=100指的是每次取100个样本训练。所以需要迭代300000/100=3000次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batchsize=3000\n",
    "$$\\theta_{j}=\\theta_{j}-\\alpha * \\frac{1}{3000} \\sum_{i}^{3000}\\left(h_{\\theta}\\left(x^{i}\\right)-y^{i}\\right) * x_{j}^{i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 优点：\n",
    "    1. 通过矩阵运算，每次在一个batch上优化神经网络参数并不会比单个数据慢太多。\n",
    "    2. 每次使用一个batch可以大大减小收敛所需要的迭代次数，同时可以使收敛到的结果更加接近梯度下降的效果。(比如上例中的30W，设置batch_size=100时，需要迭代3000次，远小于随机梯度下降的30W次)\n",
    "    3. 可实现并行化。\n",
    "- 缺点：\n",
    "    1. batch_size的不当选择可能会带来一些问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 底层实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:53:30.500185Z",
     "start_time": "2020-03-11T04:53:30.466316Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "n7sjhbiVrzlZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"初始化Linear Regression模型\"\"\"\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = None\n",
    "        self._theta = None\n",
    "    \n",
    "    #批量梯度下降\n",
    "    def fit_gd(self, X_train, y_train, eta=0.001, n_iters=1e4):\n",
    "        \"\"\"根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型\"\"\"\n",
    "        assert X_train.shape[0] == y_train.shape[0], \\\n",
    "            \"the size of X_train must be equal to the size of y_train\"\n",
    "        \n",
    "        #求损失函数\n",
    "        def J(theta, X_b, y):\n",
    "            try:\n",
    "                return np.sum((y - X_b.dot(theta)) ** 2) / len(y)\n",
    "            except:\n",
    "                return float('inf')\n",
    "        \n",
    "        #求导损失函数\n",
    "        def dJ(theta, X_b, y):\n",
    "            return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)\n",
    "        \n",
    "        #运行梯度下降\n",
    "        def gradient_descent(X_b, y, initial_theta, eta, n_iters=1e4, epsilon=1e-8):\n",
    "            theta = initial_theta\n",
    "            cur_iter = 0\n",
    "            while cur_iter < n_iters:\n",
    "                gradient = dJ(theta, X_b, y)\n",
    "                last_theta = theta\n",
    "                theta = theta - eta * gradient\n",
    "                if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):\n",
    "                    break\n",
    "                cur_iter += 1\n",
    "            return theta\n",
    "\n",
    "        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n",
    "        initial_theta = np.zeros(X_b.shape[1])\n",
    "        self._theta = gradient_descent(X_b, y_train, initial_theta, eta, n_iters)\n",
    "        self.intercept_ = self._theta[0]\n",
    "        self.coef_ = self._theta[1:]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #随机梯度下降\n",
    "    def fit_sgd(self, X_train, y_train, n_iters=5, t0=5, t1=50):\n",
    "        \"\"\"根据训练数据集X_train, y_train, 使用梯度下降法训练Linear Regression模型\"\"\"\n",
    "        assert X_train.shape[0] == y_train.shape[0], \\\n",
    "            \"the size of X_train must be equal to the size of y_train\"\n",
    "        assert n_iters >= 1\n",
    "\n",
    "        def dJ_sgd(theta, X_b_i, y_i):\n",
    "            return X_b_i * (X_b_i.dot(theta) - y_i) * 2.\n",
    "\n",
    "        def sgd(X_b, y, initial_theta, n_iters, t0=5, t1=50):\n",
    "\n",
    "            def learning_rate(t):\n",
    "                return t0 / (t + t1)\n",
    "            theta = initial_theta\n",
    "            m = len(X_b)\n",
    "\n",
    "            for cur_iter in range(n_iters):\n",
    "                \n",
    "                #随机选取一个数据\n",
    "                indexes = np.random.permutation(m)\n",
    "                X_b_new = X_b[indexes]\n",
    "                y_new = y[indexes]\n",
    "                for i in range(m):\n",
    "                    gradient = dJ_sgd(theta, X_b_new[i], y_new[i])\n",
    "                    theta = theta - learning_rate(cur_iter * m + i) * gradient\n",
    "            return theta\n",
    "\n",
    "        X_b = np.hstack([np.ones((len(X_train), 1)), X_train])\n",
    "        initial_theta = np.random.randn(X_b.shape[1])\n",
    "        self._theta = sgd(X_b, y_train, initial_theta, n_iters, t0, t1)\n",
    "\n",
    "        self.intercept_ = self._theta[0]\n",
    "        self.coef_ = self._theta[1:]\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #预测\n",
    "    def predict(self, X_predict):\n",
    "        \"\"\"给定待预测数据集X_predict，返回表示X_predict的结果向量\"\"\"\n",
    "        assert self.intercept_ is not None and self.coef_ is not None, \\\n",
    "            \"must fit before predict!\"\n",
    "        assert X_predict.shape[1] == len(self.coef_), \\\n",
    "            \"the feature number of X_predict must be equal to X_train\"\n",
    "\n",
    "        X_b = np.hstack([np.ones((len(X_predict), 1)), X_predict])\n",
    "        return X_b.dot(self._theta)\n",
    "    \n",
    "    #准确率\n",
    "    def score(self, X_test, y_test):\n",
    "        \"\"\"根据测试数据集 X_test 和 y_test 确定当前模型的准确度\"\"\"\n",
    "        y_predict = self.predict(X_test)\n",
    "        return r2_score(y_test, y_predict)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"LinearRegression()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:53:32.315348Z",
     "start_time": "2020-03-11T04:53:32.280990Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lJhfd6a-rzlg"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x = boston.data\n",
    "y= boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:53:33.578813Z",
     "start_time": "2020-03-11T04:53:33.181102Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "gPjAhvSjrzlm",
    "outputId": "3ee478b4-0172-4ad8-8daf-b1913d1d42ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standerscaler = StandardScaler()\n",
    "x = standerscaler.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "gradient_descent = LinearRegression()\n",
    "gradient_descent.fit_gd(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T04:53:36.140580Z",
     "start_time": "2020-03-11T04:53:36.134988Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "BBaZSKaMrzlt",
    "outputId": "c87da593-6756-47be-a2be-8fdb3ecf81c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.763488198838198"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_descent.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V0Ckwqs-rzlx"
   },
   "source": [
    "## sklearn实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T05:04:46.443706Z",
     "start_time": "2020-03-11T05:04:46.428723Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ANwwaHkPrzlz"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "x = boston.data\n",
    "y= boston.target\n",
    "x = x[y <50.0]\n",
    "y = y[y <50.0]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2)\n",
    "\n",
    "standerscaler = StandardScaler()\n",
    "standerscaler.fit(x_train)\n",
    "x_train_scaler = standerscaler.transform(x_train)\n",
    "x_test_scaler = standerscaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-11T05:04:50.187845Z",
     "start_time": "2020-03-11T05:04:50.176998Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zJEpuxNHrzl3",
    "outputId": "8b3379c2-2117-4ed7-9942-39499aeba001"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.716269351692778"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = SGDRegressor(tol=1e-4)\n",
    "lin_reg.fit(x_train_scaler,y_train)\n",
    "lin_reg.score(x_test_scaler,y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "梯度下降算法.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
